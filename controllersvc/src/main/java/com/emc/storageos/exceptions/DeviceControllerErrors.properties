jobFailedOp=An error occurred while executing the job, Op: {0}
jobFailedOpMsg=An error occurred while executing the job, Op: {0} with message {1}
jobFailed=Operation failed due to the following error: {0}
jobFailedMsg=Operation failed due to the following error: {0} with message {1}
createVolumesFailed=Failed to create volumes {0}, Op: {1} with message {2}
createVolumesAborted=Create volumes order aborted {0}, with message {1}
deleteVolumesAborted=Delete volumes order aborted {0}, with message {1}
expandVolumeFailed=Failed to expand volume {0}, Op: {1} with message {2}
restoreVolumeFromSnapshotFailed=Failed to restore volume {0} from snapshot {1}, Op: {2} with message {3}
deleteVolumesFailed=Failed to delete volumes {0}, Op: {1} with message {2}
deleteVolumesFailedInactive=Failed to delete volume, volume {0} does not exist or is deleted already
deleteVolumeStepFailedExc=Delete Volume Step Failed for volumes {0}
exportGroupOpInitInOtherMaskError=Failed to add initiator {0} because it is already in mask {1}
exportHasExistingVolumeWithRequestedHLU=Export request for volume {0} with HLU {1} is conflicting with an existing volume. Use a different HLU for the volume in the export request.
groupCopyToTargetNotApplicable=Group copy to target operation is not applicable for non-\
   consistency group snapshot
unableToScheduleJob=Unable to schedule the job of type {0}
unableToExecuteJob=Operation failed due to the following error: {0}
failedToAcquireScanningLock=Failed to acquire Scanning lock
unforeseen=An unforeseen error occurred
unsupportedOperationOnDevType=Operation {0} is not supported on device type {1}
initiatorsWithDifferentOSType=All initiators should belong to hosts with the same OS
mixingClusteredAndNonClusteredInitiators=All initiators should belong to the same \
  cluster or not have a cluster name at all
nonClusterExportWithInitiatorsInDifferentExistingIGs=A non-cluster export was attempted \
   with initiators that are already exported to on the array. These initiators are on \
  different InitiatorGroups, and hence conflicts with the expected export paradigm for \
  ViPR
existingInitiatorGroupHasDifferentPorts=While attempting an export, an existing \
  InitiatorGroup, {0}, was found on the array with one or more of the initiators, \
  however the InitiatorGroup has other initiators in it. Using this InitiatorGroup \
  would expose volumes to hosts that were not intended to be exposed to, \
  hence this is not allowed
existingInitiatorGroupDoesNotHaveSamePorts=While attempting an export, an existing \
  InitiatorGroup, {0}, was found on the array with one or more of the initiators, \
  however the InitiatorGroup has different initiators in it. Using this InitiatorGroup \
  would expose volumes to hosts that were not intended to be exposed to, \
  hence this is not allowed
vmaxStorageGroupNameNotFound=While attempting do an export operation, \
  the VMAX storage group {0} was no longer found on the array. It could have been \
  deleted outside of ViPR.
vmaxExportGroupCreateError=Encountered an error in export create: {0}
cannotMultiExportVolumesWithHostIOLimit=ViPR Controller does not support multiple exports for VMAX3 volume with Host IO Limit set. \
  Storage Groups with Host IO Limits set: {0}, Volumes: {1}
existingMaskFoundDuringBootVolumeExport=Boot volume export on host {1} failed because there are existing export masks with volumes on the array.\n\
  \n\
  Details: \n\
  \n\
  An attempt was made to mask a boot volume to host {1} on the array.  This requires a clean array mask for {1}.\n\
  However existing array mask(s) {0} were found that contain the initiator WWNs associated with the new host.\n\
  Attempting to export this boot volume will result in a multi-volume export to a fresh host, resulting in unpredictable\n\
  behavior when the host is changed to boot from SAN.\n\
  \n\
  This is a side-effect of a new compute service profile re-using virtual HBA initiator WWNs from a previous compute deployment where\n\
  the existing masks and zones were not cleaned properly on the array and switch, respectively.\n\
  \n\
  Remediation options are as follows:\n\
  \n\
  1. Remove array masks {0} from array manually and retry operation.\n\
  2. Contact EMC Support for additional remediation paths.
existingMaskFoundDuringBootVolumeExportXio=Boot volume export on host {1} failed because there are existing initiator group(s) with volumes on the array.\n\
  \n\
  Details: \n\
  \n\
  An attempt was made to mask a boot volume to host {1} on the array.  This requires a clean array initiator group for {1}.\n\
  However existing array mask(s) {0} were found that contain the initiator WWNs associated with the new host.\n\
  Attempting to export this boot volume will result in a multi-volume export to a fresh host, resulting in unpredictable\n\
  behavior when the host is changed to boot from SAN.\n\
  \n\
  This is a side-effect of a new compute service profile re-using virtual HBA initiator WWNs from a previous compute deployment where\n\
  the existing masks and zones were not cleaned properly on the array and switch, respectively.\n\
  \n\
  Remediation options are as follows:\n\
  \n\
  1. Remove or clean initiator group(s) {0} from array manually and retry operation.\n\
  2. Contact EMC Support for additional remediation paths.
vmaxFASTStorageGroupAlreadyPartOfExistingMaskingView=While attempting to export a FAST volume, \
an existing Storage Group {0} was found on the array with non-FAST volumes in it. \
Adding FAST volumes to this Storage Group is not permissible.
concurrentRemoveFromSGCausesEmptySG=Attempt to concurrently remove volumes from a storage group would lead  \
to an empty storage group. This is not allowed on the array. Please retry the request.
invalidURI=Invalid URI.
unableToDeleteIGs=Unable to delete Initiator Group: {0}
xtremioInitiatorGroupsNotDetected=Failed to detect Initiator Groups for creating lun maps,as registering initiators on XtremIO Array {0} failed
changeVirtualPoolFailed=Failed to change virtual pool for volumes {0}, Op: {1} with message {2}
changeVirtualArrayFailed=Failed to change virtual array for volumes {0}, Op: {1} with message {2}
vmaxMaskSupportsSingleHostError=VMAX masking view with initiator group {0} only supports a single host (non-cascading initiator group).  \
  In order for ViPR to utilize this existing masking view for a multi-host or cluster mask, manually convert this mask to use a cascading \
  initiator group with two or more hosts.
createFileSharesFailed=Failed to create FileShares {0}, Op: {1} with message {2}
expandFileShareFailed=Failed to expand FileShares {0}, Op: {1} with message {2}
reduceFileShareFailed=Failed to reduce the quota for FileShares  {0}, Op: {1} with message {2}
deleteFileSharesFailed=Failed to delete FileShares {0}, Op: {1} with message {2}
volumeReachedMaxExports=Unable to export volume {0} on HLU {1} as it has reached max number of exports per Host/Cluster. Message : {2}
exportFileShareFailed=Failed to export File System {0}, Op: {1} with message {2}
unableToPerformFileOperationDueToInvalidObjects=Failed to perform {0} on file system objects {1} due to {2}
updateFileShareExportRulesFailed=Failed to update export rules of File System {0}, Op: {1} with message {2}
createFileSystemSnapshotFailed=Failed to create Snapshot of File System {0}, Op: {1} with message {2}
deleteExportRuleFailed=Failed to delete File System export rule  {0}, Op: {1} with message {2}
deleteCIFSShareFailed=Failed to delete File System CIFS share {0}, Op: {1} with message {2}
restoreFSFromSnapshotFailed=Failed to restore file system from snapshot {0}, Op: {1} with message {2}
deleteFSSnapshotFailed=Failed to delete file system snapshot {0}, Op: {1} with message {2}
deleteShareACLFailed=Failed to delete file system share ACL {0}, Op: {1} with message {2}
removeVolumeFromMaskFailed=Volume {0} nativeId/alternateName is null.
updateFileShareCIFSACLsFailed=Failed to update file system share access control list {0}, Op: {1} with message {2}
updateFileShareNFSACLFailed=Failed to update file system NFS access control list {0}, Op: {1} with message {2}
unableToUpdateFileSystem=Update File System encountered an internal error - Operation: {0} with message: {1}
noNasServerFoundToAddStepsToApplyPolicy=No NAS Servers found on storage system {0}
assignFilePolicyFailed={0}: Assign file policy at {1} failed. Reason: {2}.
unassignFilePolicyFailed={0}: Unassign file policy failed: Reason {1}.
replicationInfoSettingFailed=Setting replication attributes failed: Reason {0}
updateFilePolicyFailed={0}: Update file policy failed: Reason {1}.
deviceProtectionPolicyOperationFailed=Policy {0}: Successfully {1} {3} storage policies and failed to update {2} policies. Please verify the task steps for complete status
fileReplicationConfFailoverOperationFailed={1} out of {2} replication configurations updated on fail over of file system {0} . Please verify the task steps for complete status
changePortGroupValidationError=Could not change port group because {0}.
