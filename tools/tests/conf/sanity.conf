#
# ViPR Sanity Configuration File
#
# This file contains configuration settings to run ViPR tests as part of the "sanity"
# suite of tests.
#
# Prerequisites:
# - A ViPR instance must be running that sanity can connect to
# - A set of hardware OR simulators for the tests to run against, if the test requires
#   hardware.
# - If the test requires hardware, a set of hardware attributes must be identified 
#   (serial numbers, IP address, credentials) for the specific test(s) you wish to run.
# - If the test requires hardware and no hardware is available, the correct simulator(s)
#   must be deployed and running.  (see comment above "HW_SIMULATOR_IP" variable below)
# - The variables in this configuration file must be carefully configured to match
#   the hardware or simulators.
#

#
# ViPR Sanity Configuration
#
SYSADMIN=root
SYSADMIN_PASSWORD=${SYSTEM_PASSWORD:-ChangeMe1!}

#
# Simulators can be used for specific test suites, such as "quick", "security"
# and "recoverpoint quick".
# Where the simulators run depends on the type of configuration
# For ViPR dev kit, there is an internal simulator at a fixed address.
# For CoprHD, simulators can be deployed on your CoprHD devkit or on a separate VM.
#
# See the wiki at:
# https://coprhd.atlassian.net/wiki/display/COP/Simulating+Hardware+for+Sanity+and+General+Testing+of+CoprHD
#
HW_SIMULATOR_IP=10.247.101.43

#
# LOCAL_LDAP_SERVER_IP contains LDAP Server IP. Provide IP of the machine
# where LDAP Simulator is running. LOCAL_LDAP_SERVER_IP is used as authentication
# provider server_urls for the sanity tests. The LDAP Simulator can be run
# either from the HW_SIMULATORS machine or the CoPRHD devkit. If LDAP Simulator is
# running from the HW_SIMULATORS machine, leave the variable LOCAL_LDAP_SERVER_IP unchanged.
# If it is running from any other machine, provide the IP of that machine
# to the variable LOCAL_LDAP_SERVER_IP.
#
LOCAL_LDAP_SERVER_IP=${HW_SIMULATOR_IP}

#
# ViPR supports both AD and LDAP as authn provider. By default all the
# sanity tests run against the LDAP (using the LDAP Simulator). To test against
# AD authn provider, there is no simulator available. Hence, we created few
# set of sanity tests that run against the AD authn provider. In order to the
# For ViPR deployment, by default AD specific sanity tests are run against
# the AD server 10.247.100.165. If you dont want to run the AD tests
# change the TEST_AD_PROVIDER to "no"(the default value is "yes").
#
TEST_AD_PROVIDER=yes

AD_AUTHN_SERVER_IP=10.247.100.165
AD_AUTHN_DOMAIN='sanity.local'

AD_AUTHN_MODE=ad
AD_AUTHN_URLS='ldap://'${AD_AUTHN_SERVER_IP}
AD_AUTHN_SEARCH_FILTER='userPrincipalName=%u'
AD_AUTHN_AUTHN_GROUP_ATTR=CN
AD_AUTHN_DOMAINS=${AD_AUTHN_DOMAIN}
AD_AUTHN_NAME=AD_Provider
AD_AUTHN_SEARCH_SCOPE=SUBTREE

AD_AUTHN_SEARCH_BASE='CN=Users,DC=sanity,DC=local'
AD_AUTHN_MANAGER_DN='CN=Administrator,'${AD_AUTHN_SEARCH_BASE}
AD_AUTHN_MANAGER_PWD='P@ssw0rd'
AD_AUTHN_WHITELIST='*Admins*,*Test*'

#
# AD tenant user mapping attribute and key.
#
AD_TENANT_ATTRIBUTE_KEY='ou'
AD_TENANT_ATTRIBUTE_VALUE='sanity'

#
# AD user configuration. This user should have the above
# attribute and its value in order to be part of the tenant.
#
AD_USER_USERNAME='super_sanity@'${AD_AUTHN_DOMAIN}
AD_USER_PASSWORD=P@ssw0rd

#
# Quick MDS configuration
#
SIMULATOR_CISCO_MDS=${HW_SIMULATOR_IP}
SIMULATOR_CISCO_MDS_USER=root
SIMULATOR_CISCO_MDS_PW=ChangeMe
 
#
# Quick SMIS configuration
#
SIMULATOR_SMIS_IP=${HW_SIMULATOR_IP}
SIMULATOR_VNX_NATIVEGUID=CLARIION+APM12310359509
SIMULATOR_VMAX_NATIVEGUID=SYMMETRIX+999595867618

#
######################### Start of File share configuration. ############################
#
# Provide the device related configuration to run test case , Based on the test case target,
# One configuration or many different device configuration can be provided. 
#
#
# Isilon configuration
#
# Provide IP Address, User name, Password and Serial no for Isilon
ISI_IP=10.247.96.132
ISI_USER=root
ISI_PASSWD=Is1l0n
ISI_SN=6805ca00ad44cad54252f514e871a3c03333

#
# VNX file device configuration
#
# Provide IP Address, Port,SMIS IP address,SMIS port,storage IP interfaces, User name, Password and Serial no for VNX file device
VNXF_IP=10.247.27.165
VNXF_PORT=443
VNXF_SMIS_IP=10.247.55.94
VNXF_SMIS_PORT=5989
VNXF_IP_ENDPOINT1=10.247.66.237
VNXF_IP_ENDPOINT2=10.247.66.236
VNXF_USER=nasadmin
VNXF_PW=nasadmin
VNXF_SN=APM00140844981

#
# Netapp 7 Mode device configuration
#
# Provide IP Address, Port, User name, Password and Serial no for NetApp 7 Mode device
NETAPPF_IP=10.247.96.204
NETAPPF_PORT=443
NETAPPF_USER=root
NETAPPF_PW=dangerous1
NETAPPF_SN=700001401764

#
# NetApp Cluster Mode device configuration
#
# Provide IP Address, Port, User name, Password and Serial no for NetApp Cluster Mode device
NETAPPCF_IP=10.247.96.56
NETAPPCF_PORT=443
NETAPPCF_USER=admin
NETAPPCF_PW=dANGEROUS1
NETAPPCF_SN=700001322607

#
# VNXE  device configuration
#
# Provide IP Address, Port, User name, Password and Serial no for VNXE device
VNXE_IP=10.247.23.106
VNXE_PORT=443
VNXE_USER=bourne
VNXE_PW=Bourn3!!
VNXE_SN=APM00144524101

#
# DataDomain file device configuration
#
# Provide IP Address, Port, User name, Password and Serial no for DataDomain device
DATADOMAINF_DEV_IP=10.247.100.17
DATADOMAINF_PORT=3009
DATADOMAINF_USER=sysadmin
DATADOMAINF_PW=abc123
DATADOMAINF_ID=d63d9c89a073550d%3A653ea2e8c8b15192

######################### End of File share configuration. ############################

#
######################### Start of VPLEX sanity configuration. ############################
#
# For VPLEX sanity, we need to set up both a VPLEX
# device and its backend arrays.  This configuration
# creates a VPLEX backend environment with two VNX arrays
# and a VMAX array.  The VNX arrays should be connected
# to the cluster-1 side of the VPLEX, and the VMAX should
# be connected to the cluster-2 side.  During ViPR
# configuration for this sanity test, two virtual arrays
# will be created, one for each side of the VPLEX (where
# director-1 ports are added to the first virtual array and
# director-2 ports are added to the second virtual array).
# Two virtual pools will also be created, one for the source
# side of the VPLEX high availability configuration and one
# for the remote side.
#
# VPLEX configuration
VPLEX_DEV_NAME=vplex_device
VPLEX_IP=10.247.96.154
VPLEX_USER=service
VPLEX_PASSWD=Mi@Dim7T
VPLEX_GUID=VPLEX+FNM00114300288:FNM00114600001
VPLEX_SMIS_USER=admin
VPLEX_SMIS_PASSWD='#1Password'
# VPLEX Backend Array Configuration
VPLEX_VNX1_NATIVEGUID=CLARIION+APM00140801303
VPLEX_VNX2_NATIVEGUID=CLARIION+APM00140844981
VPLEX_VMAX_NATIVEGUID=SYMMETRIX+000198700406
VPLEX_VNX1_SMIS_DEV_NAME=VPLEX-VNX1-PROVIDER
VPLEX_VNX2_SMIS_DEV_NAME=VPLEX-VNX2-PROVIDER
VPLEX_VMAX_SMIS_DEV_NAME=VPLEX-VMAX-PROVIDER
VPLEX_VNX1_SMIS_IP=10.247.99.68
VPLEX_VNX2_SMIS_IP=10.247.99.25
VPLEX_VMAX_SMIS_IP=10.247.99.74
# VPLEX export test suite configuration needs:
# 1. the names of the networks in which the VPLEX clusters are connected
# 2. fragments for grepping the front end VPLEX port WWNs in order to find
#    ports exclusive to either the cluster-1 (CL1) or cluster-2 (CL2)
CLUSTER1NET_NAME=losam082
CLUSTER2NET_NAME=vplex154nbr2
VPLEXCL1_PORTID_GREP="7D:C4"
VPLEXCL2_PORTID_GREP="03:7D"

######################### End of VPLEX configuration. ############################

#
######################### Start of VNX and VMAX Block configuration. ############################
#

# SMIS configuration
#
VNX_SMIS_DEV=VNX-PROVIDER
VMAX_SMIS_DEV=VMAX-PROVIDER
VMAX_SMIS_IP=${VMAX_SMIS_IP:-10.247.99.71}
VNX_SMIS_IP=${VNX_SMIS_IP:-10.247.99.68}	# Bourne Controllers
SMIS_USER=admin
SMIS_PASSWD='#1Password'

#
# VNX block device configuration
#
VNXB_DEV=vnxb_device
VNXB_IP=losat168.lss.emc.com
VNXB_USER=admin
VNXB_PW=Danger0us1
VNXB_SN=${VNXB_SN:-APM00140801303}
VNXB_NATIVEGUID=CLARIION+$VNXB_SN

#
# VMAX block device configuration
#
VMAX_DEV=vmax_dev
VMAX_USER=admin
VMAX_PW=Danger0us1
VMAX_SN=${VMAX_SN:-000195701573}
VMAX_NATIVEGUID=SYMMETRIX+$VMAX_SN
VMAX_PORTS_A="FA-8E FA-7E SE-7G SE-8G"
VMAX_PORTS_B="FA-8F FA-7F SE-7H SE-8H"

#
# BROCADE configuration
#
BROCADE_NETWORK=lglw9250
BROCADE_IP=10.247.99.250
BROCADE_USER=administrator
BROCADE_PW=password

# SRDF Configuration (R1 Side)
#
SRDF_VMAXA_SMIS_DEV=SRDF-VMAX-1
SRDF_VMAXA_SMIS_IP=10.247.99.71
SRDF_VMAXA_NATIVEGUID=SYMMETRIX+000195701573
SRDF_VMAXA_STORAGEPORTS="50:00:09:73:00:18:95:1C 50:00:09:73:00:18:95:1D 50:00:09:73:00:18:95:5C 50:00:09:73:00:18:95:5D"
SRDF_VMAXA_VSAN=FABRIC_losam082-fabric

#
# SRDF Configuration (R2 Side)
#
SRDF_VMAXB_SMIS_DEV=SRDF-VMAX-2
SRDF_VMAXB_SMIS_IP=10.247.99.72
SRDF_VMAXB_NATIVEGUID=SYMMETRIX+000195701505
SRDF_VMAXB_STORAGEPORTS="50:00:09:73:00:17:85:1C 50:00:09:73:00:17:85:1D 50:00:09:73:00:17:85:5C 50:00:09:73:00:17:85:5D"
SRDF_VMAXB_VSAN=FABRIC_losam082-fabric

#
# VNX block device configuration
#
VNXS_DEV=vnxblock_dev
VNXS_USER=admin
VNXS_PW=Danger0us1

######################### End of VNX and VMAX Block configuration. ############################

#
######################### Start of RecoverPoint configuration. ############################
#
# RecoverPoint can be run in a simulated and physical environment.
#

# TODO add env variable to alternate between configs
RP_CONFIG_NUMBER=1

if [ ${RP_CONFIG_NUMBER} -eq 1 ]
then

	echo "Using RP configuration #1"

	#
	# RecoverPoint configuration
	#
	RP_USE_MGMT_IP=lrmb017.lss.emc.com

	#
	# RecoverPoint Configuration for VNX
	#
	RP_USE_VNX_SMIS_IP=$VPLEX_VNX1_SMIS_IP
	RP_USE_VNXB_NATIVEGUID=$VPLEX_VNX1_NATIVEGUID

	# RecoverPoint Configuration for VMAX
	#
	RP_USE_VMAXB_SMIS_IP=$VPLEX_VMAX_SMIS_IP
	RP_USE_VMAXB_NATIVEGUID=$VPLEX_VMAX_NATIVEGUID

elif [ ${RP_CONFIG_NUMBER} -eq 2 ]
then

	echo "Using RP configuration #2"

	#
	# RecoverPoint configuration
	#
	RP_USE_MGMT_IP=lrmb016.lss.emc.com

	#
	# RecoverPoint Configuration for VNX
	#
	RP_USE_VNX_SMIS_IP=$VPLEX_VNX1_SMIS_IP
	RP_USE_VNXB_NATIVEGUID=$VPLEX_VNX1_NATIVEGUID

	# RecoverPoint Configuration for VMAX
	#
	RP_USE_VMAXB_SMIS_IP=$VPLEX_VMAX_SMIS_IP
	RP_USE_VMAXB_NATIVEGUID=$VPLEX_VMAX_NATIVEGUID

fi

RP_IP=$RP_USE_MGMT_IP
RP_USER=admin
RP_PASSWORD=admin
RP_PORT=7225

RP_BROCADE_NETWORK=$BROCADE_NETWORK
RP_BROCADE_NETWORK_PROVIDER=$BROCADE_IP
RP_BROCADE_NETWORK_PROVIDER_USERNAME=$BROCADE_USER
RP_BROCADE_NETWORK_PROVIDER_PASSWORD=$BROCADE_PW

RP_VSAN=FABRIC_losam082-fabric

#
# Configuration for Simulator
#
SIMULATOR_IP=${HW_SIMULATOR_IP}
VMAX1_SIMULATOR_NATIVEGUID=SYMMETRIX+999000414197
VMAX2_SIMULATOR_NATIVEGUID=SYMMETRIX+999024668923
VPLEX_SMIS_USER=admin
VPLEX_SMIS_PASSWD='#1Password'
VPLEX_SIMULATOR_GUID=VPLEX+FNM00130900544:FNM00130900545

RP_FABRIC_SIM_USER=user
RP_FABRIC_SIM_PW=ChangeMe

RP_HOST_USER=user
RP_HOST_PW=password

######################### End of RecoverPoint configuration. ############################


#
######################### Start of HDS Block configuration. ############################
#

#
# HiCommand Storage Provider configuration
#
HDS_PROVIDER=${HDS_PROVIDER:-lglak148}
HDS_PROVIDER_IP=${HDS_PROVIDER_IP:-10.247.62.148}
HDS_PROVIDER_PORT=2001
HDS_PROVIDER_USER=system
HDS_PROVIDER_PASSWD=manager
HDS_PROVIDER_INTERFACE_TYPE=hicommand

######################### End of HDS Block configuration. ############################

#
######################### Start of XtremIO Block configuration. ############################
#

#
# XtremIO configuration
#
XTREMIO_IP=10.247.63.50
XTREMIO_USER=Vipradmin
XTREMIO_PASSWD=Bourn3!!
XTREMIO_SN=APM00141017919

XTREMIO_STORAGE_PORTS="21:00:00:24:FF:57:7F:FB 21:00:00:24:FF:57:A8:BB 21:00:00:24:FF:57:A9:A3 21:00:00:24:FF:57:A9:B7"
XTREMIO_INI="50:00:50:56:9F:3B:41:A1 50:00:50:56:9F:4F:85:A1 50:00:50:56:9F:4F:85:A2 50:00:50:56:9F:4F:85:A3 50:00:50:56:9F:4F:85:A4 50:00:50:56:9F:4F:85:C1"

XTREMIO2_IP=10.247.13.131
XTREMIO2_SN=APM00142114518
XTREMIO2_STORAGE_PORTS=""
XTREMIO_INI=""

######################### End of XtremIO Block configuration. ############################

#
# system setup and syssvc sanity configuration
# export to make them available to the python script
# 
# comma-separated DNS server list in IPv4 and IPv6 
export DNS_PROP_VALUE="10.247.96.132,10.254.66.23,10.254.66.24"
export DNS_PROP_VALUE6="2620:0:170:2858:5e4:4c7f:8b02:945"
 
# comma-separated NTP server list in IPv4 and IPv6
export NTP_PROP_VALUE="10.254.140.21,10.254.140.22"
export NTP_PROP_VALUE6="2620:0:170:2842::7151"

# Allowed values:
#   coprhd_devkit
#   vipr_appliance
#   vipr_devkit
export DEPLOYMENT_TYPE="vipr_appliance"

# configuration for connectemc email - SMTP
export SMTP_PRIMARY_EMAIL="emailalertesg@emc.com"
export SMTP_SERVER="mailhub.lss.emc.com"
export SMTP_SENDER_EMAIL="DONOTREPLY@emc.com"

# configuration for connectemc email - FTPS
export FTPS_IP_ADDRESS="10.15.54.102"

export LICENSE_FILE='INCREMENT ViPR_Block EMCLM 2.0 permanent uncounted 
VENDOR_STRING=CAPACITY=1024;CAPACITY_UNIT=TB;SWID=PXTYD1DZK59Y4C;PLC=VIPR; 
HOSTID=ANY dist_info="ACTIVATED TO 49ers Inn" ISSUER=EMC 
ISSUED=10-Jan-2014 NOTICE="ACTIVATED TO License Site Number: 
PTA06JUN20131086059" SN=2162734 SIGN="003D CF8A 7CED 90ED 318E 
47C9 001A F400 3A5D EEE7 81F0 704C CBDA 2F32 0745" 
INCREMENT ViPR_Controller EMCLM 2.0 permanent uncounted 
VENDOR_STRING=CAPACITY=1024;CAPACITY_UNIT=TB;SWID=PXTYD1DZK59Y4C;PLC=VIPR; 
HOSTID=ANY dist_info="ACTIVATED TO 49ers Inn" ISSUER=EMC 
ISSUED=10-Jan-2014 NOTICE="ACTIVATED TO License Site Number: 
PTA06JUN20131086059" SN=2162734 SIGN="00EC 6B99 FB75 280D B932 
75DD 21D1 EC00 5634 5848 462F 7ACD 0032 5081 2923"'
	
export SYSTEM_UPDATE_CATALOG_URL="https://colu-test.emc.com/soap/rpc"

export SYSTEM_UPDATE_DIRECTORY_URL="http://lglaf020.lss.emc.com/ovf/Bourne/"
export SYSTEM_UPDATE_DIRECTORY_URL6="http://[2620:0:170:2842:250:56ff:fe9f:28ef]:8081/workspace/imgs"
