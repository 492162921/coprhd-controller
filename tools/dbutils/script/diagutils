#!/bin/bash
# Copyright (c) 2014 EMC Corporation
# All Rights Reserved
#
# This software contains the intellectual property of EMC Corporation
# or is licensed to EMC Corporation from third parties.  Use of this
# software and the intellectual property contained therein is expressly
# limited to the terms and conditions of the License Agreement under which
# it is provided by or on behalf of EMC.
#

DIAGCOLLECT_DIR="/tmp/diagutils"
DIAGCOLLECT_MAME="diagutils-`date +%Y%0m%0d%0k%0M%0S`"
DIAGCOLLECT_ARCHIVE_NAME="${DIAGCOLLECT_MAME}"
DIAGCOLLECT_OUTPUT="${DIAGCOLLECT_DIR}/${DIAGCOLLECT_MAME}"
COMMAND_DIR="/opt/storageos/bin"
MIN_CFS=(BlockConsistencyGroup BlockMirror BlockSnapshot Cluster ExportGroup ExportMask FCZoneReference Host Initiator Network NetworkSystem ProtectionSet ProtectionSystem StoragePort StorageSystem VirtualArray VirtualDataCenter VirtualPool Volume)

#This script is used to help user to restore a backupset simply
usage() {
    echo "Usage:"
    echo "       $0 <-all|-quick>|<-min_cfs|-all_cfs|-zk|-backup|-logs|-properties|-health> [-ftp <server name or ip and directory> -u <user name> -p <password>]"
    echo "Options:"
    echo "       -min_cfs               Collect column families through output of dbutils list and/or cqlsh"
    echo "                              minimal column families include(could be reconfigured by editing $0):"
    echo "                              VirtualPool, VirtualDataCenter, Volume, ExportMask, ExportGroup, Host, Cluster, StoragePort, StorageSystem, Initiator, FCZoneReference, Network,"
    echo "                              NetworkSystem, Vcenter, ProtectionSet, BlockConsistencyGroup, VirtualArray, StoragePool, and RelationIndex and AltIndex."
    echo "       -all_cfs               Collect all column families through output of dbutils list and/or cqlsh."
    echo "       -zk                    Collect zk jobs and queues through zkutils."
    echo "       -backup [backup name]  Create a new ViPR system backup/dump of DB and ZK through bkutils, which can be restored later."
    echo "                              If the backup name is not specified, timestamp will be used instead."
    echo "                              If the backup name already exists, the utility won't create it and just copy it into the archive."
    echo "       -logs                  Collect all system logs (/var/log/messages) and ViPR logs including the rotated ones."
    echo "       -properties            Collect system properties (version, node count, node names, etc.)."
    echo "       -health                Collect system health information (e.g. node and service status, etc.), performance data of local node from top output."
    echo "       -all                   Including the output gathered by options: backup (with default backup name), zk, logs, properties, and health."
    echo "       -quick                 Including the output gathered by options: minimal column families, zk, logs, properties and health."
    echo "       -ftp <server name or ip and directory> -u <user name> -p <password>"
    echo "                              If specified, the output will be transferred to the external ftp site"
    echo "                              Note: It's suggested to always dump the output to the FTP to retain space in ViPR nodes and not run into issue due to /tmp folder being full"
    echo "For example:"
    echo "       $0 -all -ftp ftp://10.247.101.11:/tmp -u usera -p xxx"
    echo "       $0 -quick"
    echo "       $0 -min_cfs -zk -logs"
    echo "Notes:"
    echo "       '-all' equal to '-backup -zk -logs -properties -health'"
    echo "       '-quick' equal to '-min_cfs -zk -logs -properties -health'" 
}

######################
# Collect Functions
######################
collect_min_cfs() {
    local param="db-cfs"
    check_if_skip $param
    if [[ ${SKIP_COLLECT} == true ]] ; then
        return
    fi

    echo "Collecting minimum cfs.."
    local minCfsDir="${DIAGCOLLECT_OUTPUT}/$param"
    mkdir -p ${minCfsDir}
    
    set +e
    diagnose_cfs ${minCfsDir} "m"
    set -e
}

collect_all_cfs() {
    local param="db-cfs"
    check_if_skip $param
    if [[ ${SKIP_COLLECT} == true ]] ; then
        return
    fi

    echo "Collecting all cfs.."
    local allCfsDir="${DIAGCOLLECT_OUTPUT}/$param"
    mkdir -p ${allCfsDir}

    set +e
    diagnose_cfs ${allCfsDir} "f"
    set -e
}

collect_zk() {
    local param="zk-info"
    check_if_skip $param
    if [[ ${SKIP_COLLECT} == true ]] ; then
        return
    fi

    echo "Collecting zk info.."
    local zkDir="${DIAGCOLLECT_OUTPUT}/$param"
    mkdir -p ${zkDir}

    set +e
    ${COMMAND_DIR}/zkutils path / > $zkDir/zk-path
    ${COMMAND_DIR}/zkutils ephemeral -withdata > $zkDir/zk-ephemeral
    set -e
}

collect_backup() {
    local backupName="${DIAGCOLLECT_MAME}"
    if [[ $# -eq 1 ]] && [[ $1 != -* ]]; then
        backupName=$1
    fi

    set +e
    local foundName=`${COMMAND_DIR}/bkutils -l | grep "${backupName}" | awk '{print $1}'`
    if [ "${foundName}" == "${backupName}" ]; then
        echo "Backup($backupName) exists"
    else
        ${COMMAND_DIR}/bkutils -c ${backupName} -f
    fi
    set -e

    echo "Collecting backup data.."
    local backupDir="${DIAGCOLLECT_OUTPUT}/backup"
    mkdir -p ${backupDir}
    collect_data "/data/backup/${backupName}" "${backupDir}" "false"
  
    set +e 
    cd ${backupDir}
    zip ${backupName}.zip * &>/dev/null
    rm -rf ${backupName}_*
    ${COMMAND_DIR}/bkutils -d ${backupName} &> /dev/null
    set -e
}

collect_logs() {
    echo "Collecting logs.."
    local logsDir="${DIAGCOLLECT_OUTPUT}/logs"
    mkdir -p ${logsDir}

    collect_data "/opt/storageos/logs" "${logsDir}" "true"
}

collect_properties() {
    echo "Collecting properties.."
    local propDir="${DIAGCOLLECT_OUTPUT}/properties"
    mkdir -p ${propDir}
 
    set +e 
    /etc/systool --getprops > $propDir/systool-getprops
    /etc/systool --get-default > $propDir/systool-getdefault
    set -e
}

collect_health() {
    echo "Collecting health infomation.."
    local healthDir="${DIAGCOLLECT_OUTPUT}/health"
    mkdir -p ${healthDir}
    
    set +e
    free > ${healthDir}/${LOCAL_NODE}-memory
    df -l > ${healthDir}/${LOCAL_NODE}-space
    ifconfig > ${healthDir}/${LOCAL_NODE}-ifconfig
    /etc/diagtool > ${healthDir}/${LOCAL_NODE}-diagtool   

    for i in $(seq 1 ${NODE_COUNT})
    do
        local viprNode=$(get_nodeid)
        ${COMMAND_DIR}/nodetool -h ${viprNode} -p 7199 status &> ${healthDir}/${viprNode}-dbstatus
        ${COMMAND_DIR}/nodetool -h ${viprNode} -p 7299 status &> ${healthDir}/${viprNode}-geodbstatus
        ${COMMAND_DIR}/nodetool -h ${viprNode} compactionhistory &> ${healthDir}/${viprNode}-compactionhistory
        ${COMMAND_DIR}/nodetool -h ${viprNode} compactionstats &> ${healthDir}/${viprNode}-compactionstats
        ${COMMAND_DIR}/nodetool -h ${viprNode} cfstats &> ${healthDir}/${viprNode}-cfstats
    done
    set -e
}

collect_all() {
    echo "We are collecting a complete set of diagnostic data.."
    collect_logs
    collect_properties
    collect_health
    collect_backup
    collect_zk
    collect_all_cfs
}

collect_quick() {
    echo "We are collecting a default set of diagnostic data..."
    collect_logs
    collect_properties
    collect_health
    collect_zk
    collect_min_cfs
}

create_archive() {
    echo -n "Creating the final archive(${DIAGCOLLECT_DIR}/${DIAGCOLLECT_ARCHIVE_NAME}.zip).."
    cd ${DIAGCOLLECT_DIR}

    local task="zip -r ${DIAGCOLLECT_ARCHIVE_NAME}.zip ${DIAGCOLLECT_MAME}/*"
    execute_with_progress_point "${task}"
    if [[ $? -eq 0 ]]; then
        rm -rf ${DIAGCOLLECT_OUTPUT}
    fi
}

upload_to_ftp() {
    local ftpserver=$1
    local user=$2
    local password=$3

    cd ${DIAGCOLLECT_DIR}
    local uploadfile=${DIAGCOLLECT_ARCHIVE_NAME}.zip
    echo -n "Uploading ${uploadfile} to ftp server(${ftpserver}).."

    local task="cat ${uploadfile} | curl -sSk -u ${user}:${password} -a -T - "${ftpserver}"/${DIAGCOLLECT_ARCHIVE_NAME}.zip"
    execute_with_progress_point "${task}"
    if [[ $? -eq 0 ]]; then
        echo "Removing local archive file.."
        rm -rf ${uploadfile}
    fi
}

######################
# Script Libs
######################
collect_data() {
    set +e
    local sourceDir=${1}
    local targetDir=${2}
    local haveSubDir=${3}
    
    for i in $(seq 1 ${NODE_COUNT})
    do
        local viprNode=$(get_nodeid)
        if [ $haveSubDir == "true" ]; then
            local targetDir="${2}/$viprNode"
            mkdir -p $targetDir
        fi
        scp -r svcuser@"$viprNode":"${sourceDir}"/* ${targetDir} &>/dev/null
    done
    wait
    set -e
}

get_nodeid() {
    if [ ${NODE_COUNT} -eq 1 ]; then
        echo "${LOCAL_NODE}"
    else
        echo "vipr$i"
    fi
}

user_confirm() {
    local message=${1}
    while true; do
        read -p "$message(yes/no)" yn
        case $yn in
            [Yy]es ) break;;
            [Nn]o )  echo "Exiting.."; exit;;
            * ) echo "Invalid input.";;
        esac
    done
}

execute_with_progress_point() {
    local task=$1

    set +e
    while [ 1 ]; do echo -n "."; sleep 5; done &
    local print_pid=$!
    local result="success"
    eval "${task}" &>/dev/null
    if [ $? -ne 0 ]; then
        result="failed"
    fi
    kill ${print_pid} &>/dev/null
    set -e

    echo "${result}"
    if [[ ${result} == "failed" ]]; then
        return 1
    fi
}

check_if_skip() {
    SKIP_COLLECT=false

    local param=$1
    if [[ ${NODE_AVAILABLE} == false ]]; then
        echo "Local node is unavailable, can not collect ${param}.."
        SKIP_COLLECT=true
        return
    fi
    if [[ ${CLUSTER_AVAILABLE} == false ]]; then
        echo "Cluster is unavailable, can not collect ${param}.."
        SKIP_COLLECT=true
    fi
}

check_node_cluster_status() {
    NODE_AVAILABLE=false
    CLUSTER_AVAILABLE=false

    for i in $(seq 1 ${NODE_COUNT})
    do
        local viprNode=$(get_nodeid)
        local tmpFile="/tmp/zk_telnet_status"
        echo ruok | curl telnet://${viprNode}:2181 &> ${tmpFile}
        cat ${tmpFile} | grep "imok" &>/dev/null
        if [ $? == 0 ]; then
            if [ ${viprNode} == ${LOCAL_NODE} ]; then
                NODE_AVAILABLE=true
            fi
            echo stat | curl telnet://${viprNode}:2181 &>${tmpFile}
            cat ${tmpFile} | grep "Mode" &>/dev/null
            if [ $? == 0 ]; then
                CLUSTER_AVAILABLE=true
            fi
        fi
        if [[ $NODE_AVAILABLE == true && $CLUSTER_AVAILABLE == true ]]; then
            break;
        fi
    done

    if [[ ${NODE_AVAILABLE} == false ]] ;then
        echo "Key service(s) on this node is unavailable, some collection would be missed, we'd better change to execute the tool on a healthy node."
        user_confirm "Are you sure you want to continue?"
    fi

    if [[ ${CLUSTER_AVAILABLE} == false ]]; then
        echo "Cluster is unavailable, some collection would be missed"
    fi
}

diagnose_cfs() {
    local outputDir=$1
    local option=$2

    #
    # Cleanup previous output
    #
    if [ -e ${outputDir} ]; then
        rm -rf ${outputDir}
    fi
    mkdir -p ${outputDir}/lists

    #
    # Generate Column Family lists
    #
    if [ "${option}" == "m" ]; then
        CFLIST_FINAL=(${MIN_CFS[*]})
    else
        CFLIST_FINAL=$($COMMAND_DIR/dbcli show_cf | cut -d " " -f3)
    fi

    #
    # No. of concurrent "dbutils list" jobs that can be done in parallel
    # set to number of CPU cores (default to 4)
    #
    cat /proc/cpuinfo | grep processor &>/dev/null
    if [ $? -eq 0 ]; then
        MAX_CURR_JOBS=`cat /proc/cpuinfo | grep processor | wc -l`
    else
        MAX_CURR_JOBS=4
    fi

    #
    # Generate the "dbutils list" output files
    # processed in parallel with:  no. of threads == no. of CPU cores
    #
    echo -n -e "\tNow generating dbutils listings, this may take several minutes..."
    NUM_CURR_JOBS=0
    for CF in ${CFLIST_FINAL[@]}; do
        for i in {1..$MAX_CURR_JOBS}; do
            nohup $COMMAND_DIR/dbutils list $CF > ${outputDir}/lists/"dbutils.list.$CF.out" 2> ${outputDir}/lists/cfs_dump.err < /dev/null &
            ((NUM_CURR_JOBS += 1))

            while [ $NUM_CURR_JOBS -ge $MAX_CURR_JOBS ]; do
                echo -n "."
                sleep 5
                NUM_CURR_JOBS=`jobs -r | wc -l`
            done
        done
    done

    #
    # Format the generated "dbutils list" output files
    # (further formatting can be added with additional "else if" clauses in the "gawk" program below)
    #
    for outfile in `ls ${outputDir}/lists/*.out` ;
    do
        gawk '(NR>2) { 
            if ((match($0, /(.+)time=(.+)/, a)) != 0) {
                match(a[2], /(^[0-9]+)(.+)/, b)
                strlength = length(b[1]) 
                epochtime1 = substr(b[1], 1, (strlength-3))
                millisecs1 = substr(b[1], (strlength-2), 3)
                print(a[1] "\n                time=" epochtime1 millisecs1 "\n                " strftime("%Y-%m-%d %H:%M:%S",epochtime1) " " millisecs1 "ms UTC\n" b[2])
            }
            else if ( (match($0,/(.+)Time = (.+)/,a) != 0) && (a[1] !~ "creation") && (a[2] !~ /^0/) )    {
                strlength = length(a[2]) 
                epochtime1 = substr(a[2], 1, (strlength-3))
                millisecs1 = substr(a[2], (strlength-2), 3)
                print(a[1] "Time = " epochtime1 millisecs1 " (" strftime("%Y-%m-%d %H:%M:%S",epochtime1) " " millisecs1 "ms UTC)")
            } 
            else if ($1 == "id:")           {
                print "\n * * * * * * * * *  * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n\n",$0;
            }
            else if ($0 ~ /^Number of All Records is:/)             {
                print "\n * * * * * * * * *  * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n\n",$0;
            }
            else {print $0}
        }' $outfile > "${outputDir}/lists/`basename $outfile .out`.txt"
    done

    echo; echo -e "\tCapturing RelationIndex output, AltIndex output and system properties..."

    #
    # Generate cqlsh RelationIndex and AltIndex output
    #
    RI_INPUT_FILE="cqlsh.RelationIndex_Input.txt"
    AI_INPUT_FILE="cqlsh.AltIdIndex-Input.txt"
    echo "SELECT * FROM \"RelationIndex\" LIMIT 1000000;" > ${outputDir}/$RI_INPUT_FILE
    echo "SELECT * FROM \"AltIdIndex\" LIMIT 1000000;" > ${outputDir}/$AI_INPUT_FILE

    RI_OUTPUT_FILE="cqlsh.RelationIndex_Output.txt"
    AI_OUTPUT_FILE="cqlsh.AltIdIndex_Output.txt"
    $COMMAND_DIR/cqlsh -k StorageOS localhost -f ${outputDir}/$RI_INPUT_FILE > ${outputDir}/lists/$RI_OUTPUT_FILE
    $COMMAND_DIR/cqlsh -k StorageOS localhost -f ${outputDir}/$AI_INPUT_FILE > ${outputDir}/lists/$AI_OUTPUT_FILE

    rm -f ${outputDir}/$RI_INPUT_FILE
    rm -f ${outputDir}/$AI_INPUT_FILE

    #
    # Clean up temporary files and zip up the output files
    #
    TAR_FILENAME="cfs-`date +%Y%0m%0d%0k%0M%0S`.tar"
    TAR_SUBFILE="unformatted_output.tar"
    export TAR_FORMAT=gnu
    tar -cf "${outputDir}/lists/$TAR_SUBFILE" --format=gnu -C "${outputDir}/lists" ${outputDir}/lists/*.out &>/dev/null
    rm -f ${outputDir}/lists/*.out
    tar -cf "${outputDir}/$TAR_FILENAME" --format=gnu -C "${outputDir}/lists" . &>/dev/null
    gzip "${outputDir}/$TAR_FILENAME"
    rm -rf ${outputDir}/lists
}

#######################
# Validate Parameters
#######################
genaral_param_count=0
plain_param_count=0
ftp_param_count=0

check_ftp_parameter() {
    local param=$1
    local next_param=$2

    if [[ ! -n "${next_param}" ]] || [[ "${next_param}" == -* ]]; then
        echo "Invalid value of '$param'"
        usage
        exit 2
    fi
}

set_backupname() {
    BACKUP_NAME=$1
    if [[ ! -n "${BACKUP_NAME}" ]] || [[ "${BACKUP_NAME}" == -* ]]; then
        BACKUP_NAME="${DIAGCOLLECT_MAME}"
    fi
}

validate_parameters() {
    if [[ ${genaral_param_count} -eq 0 ]] && [[ ${plain_param_count} -eq 0 ]]; then
        echo "Lack of mandatory paramter"
        usage
        exit 2
    fi

    if [[ ${genaral_param_count} -gt 1 ]] || [[ (${genaral_param_count} -ne 0) && (${plain_param_count} -ne 0) ]]; then
        echo "'-all/quick' could not be executed together with other commands"
        usage
        exit 2
    fi

    if [[ ${ftp_param_count} -ne 0 && ${ftp_param_count} -ne 3 ]]; then
        echo "Lack of parameters for ftp server.."
        usage
        exit 2
    fi
}

if [ $# -eq 0 ]; then
    usage
    exit 2
fi

if [ "$1" == "--help" -o "$1" == "-h" -o "$1" == "-help" ]; then
    usage
    exit 0
fi

for i in $(seq 1 $#)
do
    eval param=\$$i
    case $param in
        -all|-quick)
            genaral_param_count=$[genaral_param_count+1]
            ;;
        -min_cfs|-all_cfs|-zk|-logs|-properties|-health)
            plain_param_count=$[plain_param_count+1]
            ;;
        -backup)
            plain_param_count=$[plain_param_count+1]
            if [[ $i -lt $# ]]; then
                eval next_param=\$$[i+1]
            fi
            set_backupname "${next_param}"
            ;;
        -ftp)
            ftp_param_count=$[ftp_param_count+1]
            if [[ $i -lt $# ]]; then
                eval FTP=\$$[i+1]
            fi
            check_ftp_parameter "$param" "$FTP"
            ;;
        -u)
            ftp_param_count=$[ftp_param_count+1]
            if [[ $i -lt $# ]]; then
                eval USER=\$$[i+1]
            fi
            check_ftp_parameter "$param" "$USER"
            ;;
        -p)
            ftp_param_count=$[ftp_param_count+1]
            if [[ $i -lt $# ]]; then
                eval PASSWORD=\$$[i+1]
            fi
            check_ftp_parameter "$param" "$PASSWORD"
            ;;
        -*)
            echo "Invalid Paramter: $param"
            usage
            exit 2;;
    esac
done

validate_parameters

###################
# Diag Collect Begin
###################
NODE_COUNT=`/etc/systool --getprops | awk -F '=' '/\<node_count\>/ {print $2}'`
LOCAL_NODE=`/etc/systool --getprops | awk -F '=' '/\<node_id\>/ {print $2}'`

check_node_cluster_status

for i in $(seq 1 $#)
do
    eval param=\$$i
    case ${param} in
        -all) collect_all; DIAGCOLLECT_ARCHIVE_NAME="${DIAGCOLLECT_ARCHIVE_NAME}${param}" ;;
        -quick) collect_quick; DIAGCOLLECT_ARCHIVE_NAME="${DIAGCOLLECT_ARCHIVE_NAME}${param}" ;;
        -min_cfs) collect_min_cfs ;;
        -all_cfs) collect_all_cfs ;;
        -zk) collect_zk ;;
        -backup) collect_backup "${BACKUP_NAME}" ;;
        -logs) collect_logs ;;
        -properties) collect_properties ;;
        -health) collect_health ;;
    esac
done

create_archive 2>/dev/null

if [[ ${ftp_param_count} -eq 3 ]]; then
    upload_to_ftp "${FTP}" "${USER}" "${PASSWORD}" 2>/dev/null
fi

echo "ViPR diag finished."
